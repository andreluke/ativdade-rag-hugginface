# LLM + RAG ‚Äî Recupera√ß√£o-Aumentada (RAG) com Hugging Face

Este reposit√≥rio demonstra uma integra√ß√£o pr√°tica entre uma LLM (modelo de linguagem) e um pipeline RAG (Retrieval-Augmented Generation). O objetivo √© enriquecer a gera√ß√£o com contexto recuperado de uma base de conhecimento local usando embeddings e busca vetorial (FAISS).

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Transformers](https://img.shields.io/badge/ü§ó-Transformers-yellow.svg)](https://huggingface.co/transformers)
[![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-green.svg)](https://github.com/facebookresearch/faiss)

## üéØ Vis√£o Geral

- **RAG**: indexa√ß√£o de documentos ‚Üí recupera√ß√£o por similaridade ‚Üí gera√ß√£o condicionada ao contexto recuperado.
- **Uso t√≠pico**: responder perguntas sobre uma base de conhecimento espec√≠fica, sumariza√ß√£o com contexto, e aplica√ß√µes de QA em dom√≠nio fechado.
- **Arquitetura**: embeddings (sentence-transformers) + √≠ndice FAISS + LLM (Transformers) para gera√ß√£o.

## ‚úÖ Funcionalidades Principais

- **Indexa√ß√£o de documentos**: chunking de textos e c√°lculo de embeddings.
- **Busca sem√¢ntica eficiente**: FAISS para recupera√ß√£o r√°pida de contexto.
- **Gera√ß√£o condicionada**: LLM que recebe o contexto recuperado para respostas mais precisas.
- **Cache de embeddings e √≠ndice**: evita recomputa√ß√£o em execu√ß√µes subsequentes.
- **Facilidade de extens√£o**: trocar modelos, bases de conhecimento e par√¢metros de busca.

## üóÇÔ∏è Estrutura do Projeto

`src/`
- `main.py` ‚Äî ponto de entrada para construir √≠ndice e executar exemplos
- `rag/chunking.py` ‚Äî fun√ß√µes para segmentar documentos
- `rag/retriever.py` ‚Äî construir/consultar √≠ndice FAISS
- `llm/model.py` ‚Äî wrapper para carregar e inferir com a LLM
- `utils/preprocessing.py` ‚Äî pr√©-processamento de texto

`data/`
- `dsm_material.txt` ‚Äî base de conhecimento de exemplo (substitua pelo seu corpus)

`requirements.txt` ‚Äî depend√™ncias do projeto

## üöÄ Instala√ß√£o e Execu√ß√£o R√°pida

### Pr√©-requisitos

- Python 3.8+
- Conex√£o com internet (para baixar modelos na primeira execu√ß√£o)
- 4GB+ de RAM recomendado (varia conforme o modelo)

### Passos

1. Clone o reposit√≥rio e entre na pasta:

```powershell
git clone https://github.com/andreluke/ativdade-rag-hugginface atividade1
cd atividade1
```

2. Crie e ative um ambiente virtual (PowerShell):

```powershell
python -m venv .venv
.\venv\Scripts\Activate.ps1
```

3. Instale depend√™ncias:

```powershell
pip install --upgrade pip
pip install -r requirements.txt
```

4. Execute o exemplo (constr√≥i √≠ndice e demonstra RAG):

```powershell
python src/main.py
```

Na primeira execu√ß√£o, o √≠ndice ser√° constru√≠do e os modelos ser√£o baixados. Em execu√ß√µes subsequentes o cache √© reutilizado.

## ‚öôÔ∏è Como Funciona (Fluxo RAG)

1. Documentos em `data/` s√£o divididos em chunks.
2. Cada chunk recebe um embedding via `sentence-transformers`.
3. Embeddings s√£o indexados com FAISS.
4. Para uma consulta, calculamos o embedding da pergunta e recuperamos os N contextos mais relevantes.
5. A LLM gera a resposta condicionada pelo contexto recuperado.

## üß© Trocar Modelos

Edite `src/main.py` ou `llm/model.py` para alterar os modelos usados:

```python
# exemplo
model_name = "sentence-transformers/all-MiniLM-L6-v2"  # embeddings
llm_model = "distilgpt2"  # ou outro modelo compat√≠vel de gera√ß√£o
```

Recomenda-se usar um modelo de gera√ß√£o que aceite prompts com contexto e que caiba nos recursos dispon√≠veis.

## üìå Boas pr√°ticas

- Mantenha o corpus atualizado e normalize texto antes da indexa√ß√£o.
- Use chunking com sobreposi√ß√£o para preservar contexto.
- Avalie trade-offs entre tamanho do modelo e lat√™ncia.

## Contribui√ß√µes

- Fork ‚Üí branch ‚Üí PR. Abra issues para sugest√µes e bugs.

## Licen√ßa

Conte√∫do de exemplo para fins educacionais.
